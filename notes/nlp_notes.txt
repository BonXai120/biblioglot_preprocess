# import stanza

# 'SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.'


# Tokenization and Sentence Segmentation
# nlp = stanza.Pipeline(lang='es', processors='tokenize')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre.\
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# for i, sentence in enumerate(doc.sentences):
#     print(f'====== Sentence {i+1} tokens =======')
#     print(*[f'id: {token.id}\ttext: {token.text}' for token in sentence.tokens], sep='\n')


# Multi-Word Token (MWT) Expansion
# nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# for token in doc.sentences[0].tokens:
#     print(f'token: {token.text}\twords: {", ".join([word.text for word in token.words])}')


# Part-of-Speech & Morphological Features
# nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# print(*[f'word: {word.text}\tupos: {word.upos}\txpos: {word.xpos}\tfeats: {word.feats if word.feats else "_"}' for sent in doc.sentences for word in sent.words], sep='\n')


# Lemmatization
# nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')


# Named Entity Recognition
# nlp = stanza.Pipeline(lang='es', processors='tokenize,ner')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# print(*[f'entity: {ent.text}\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\n')

# nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma,ner')
# doc = nlp('SERÍAN las diez de la mañana de un día de octubre. \
# En el patio de la Escuela de Arquitectura, grupos de estudiantes esperaban a que se abriera la clase. \
# De la puerta de la calle de los Estudios que daba a este patio, iban entrando muchachos jóvenes que, al encontrarse reunidos, se saludaban, reían y hablaban.')
# print(
#     *[f'entity: {ent.text}\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\n')
# print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
# print(*[f'word: {word.text}\tupos: {word.upos}\txpos: {word.xpos}\tfeats: {word.feats if word.feats else "_"}' for sent in doc.sentences for word in sent.words], sep='\n')
# for i, sentence in enumerate(doc.sentences):
#     print(f'====== Sentence {i+1} tokens =======')
#     print(
#         *[f'id: {token.id}\ttext: {token.text}' for token in sentence.tokens], sep='\n')
